# -*- coding: utf-8 -*-
"""FinalMLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eOKciDEe1uXPreKiom_s7_hZoEBFdLGm
"""

import os

# Extract your real API key (remove the KGAT_ prefix!)
os.environ["KAGGLE_USERNAME"] = "davidludemann"
os.environ["KAGGLE_KEY"] = "b0b3bce7e79f0e1028621d7fcb2d7546"

# Delete old files to prevent conflicts
!rm -f ~/.kaggle/kaggle.json
!rm -f ~/.config/kaggle/kaggle.json

print("Kaggle credentials set via environment variables. Ready!")

!kaggle competitions list

!kaggle competitions download -c nlp-getting-started -p /content

import zipfile

with zipfile.ZipFile('/content/nlp-getting-started.zip', 'r') as z:
    z.extractall('/content')

print("Dataset downloaded & extracted.")

import pandas as pd

train_df = pd.read_csv('/content/train.csv')
test_df  = pd.read_csv('/content/test.csv')

train_df.head()

from sklearn.model_selection import train_test_split

X = train_df["text"]
y = train_df["target"]

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

tfidf = TfidfVectorizer(max_features=5000, stop_words="english")

X_train_tfidf = tfidf.fit_transform(X_train)
X_val_tfidf   = tfidf.transform(X_val)

logreg = LogisticRegression(max_iter=1000, solver="liblinear")
logreg.fit(X_train_tfidf, y_train)

lr_pred = logreg.predict(X_val_tfidf)

accuracy_lr  = accuracy_score(y_val, lr_pred)
precision_lr = precision_score(y_val, lr_pred)
recall_lr    = recall_score(y_val, lr_pred)
f1_lr        = f1_score(y_val, lr_pred)

print("=== Logistic Regression (TF-IDF) ===")
print("Accuracy:", accuracy_lr)
print("Precision:", precision_lr)
print("Recall:", recall_lr)
print("F1 Score:", f1_lr)

import torch
from transformers import DistilBertTokenizer

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

def encode_batch(texts):
    return tokenizer(
        texts.tolist(),
        truncation=True,
        padding=True,
        max_length=128,
        return_tensors="pt"
    )

train_enc = encode_batch(X_train)
val_enc   = encode_batch(X_val)

train_labels = torch.tensor(y_train.values)
val_labels   = torch.tensor(y_val.values)

from torch.utils.data import DataLoader, TensorDataset

train_dataset = TensorDataset(
    train_enc["input_ids"], train_enc["attention_mask"], train_labels
)

val_dataset = TensorDataset(
    val_enc["input_ids"], val_enc["attention_mask"], val_labels
)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=32)

config = {
    "epochs": 50,
    "train_batch_size": 16,
    "eval_batch_size": 32,
    "learning_rate": 1e-5,
    "weight_decay": 0.01,
    "warmup_ratio": 0.20,          # increased warmup
    "gradient_accumulation_steps": 4,
    "max_grad_norm": 1.0,
    "use_amp": True,
    "early_stopping_patience": 14, # a little longer
    "max_length": 160,
    "label_smoothing": 0.05        # reduced from 0.10
}

from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm.auto import tqdm
import torch
from torch.nn.functional import cross_entropy
from sklearn.metrics import f1_score, accuracy_score

# Load training config
epochs = config["epochs"]
accum_steps = config["gradient_accumulation_steps"]
lr = config["learning_rate"]
weight_decay = config["weight_decay"]
warmup_ratio = config["warmup_ratio"]
max_grad_norm = config["max_grad_norm"]
use_amp = config["use_amp"]
patience = config["early_stopping_patience"]
label_smoothing = config.get("label_smoothing", 0.0)  # NEW

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Custom loss function with label smoothing
def compute_loss(logits, labels):
    return cross_entropy(
        logits,
        labels,
        label_smoothing=label_smoothing
    )

# Optimizer + scheduler
optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

total_steps = int(len(train_loader) // accum_steps * epochs)
warmup_steps = int(total_steps * warmup_ratio)

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# AMP scaler
scaler = torch.cuda.amp.GradScaler(enabled=use_amp)

# Early stopping
best_val_f1 = 0.0
best_epoch = 0
no_improve = 0

print(" Starting DistilBERT fine-tuning...")

# ---------------------- TRAINING LOOP -----------------------------

for epoch in range(1, epochs + 1):
    model.train()
    optimizer.zero_grad()

    train_loop = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs} Training")

    for step, batch in enumerate(train_loop, start=1):
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        with torch.cuda.amp.autocast(enabled=use_amp):
            outputs = model(
                input_ids,
                attention_mask=attention_mask
            )
            logits = outputs.logits

            # custom loss with label smoothing
            loss = compute_loss(logits, labels) / accum_steps

        scaler.scale(loss).backward()

        if step % accum_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            scheduler.step()

        train_loop.set_postfix({"loss": loss.item() * accum_steps})

    # ---------------------- VALIDATION -----------------------------

    model.eval()
    preds, trues = [], []

    val_loop = tqdm(val_loader, desc="Evaluating")

    with torch.no_grad():
        for batch in val_loop:
            input_ids, attention_mask, labels = [b.to(device) for b in batch]
            logits = model(input_ids, attention_mask=attention_mask).logits

            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
            trues.extend(labels.cpu().numpy())

    val_f1 = f1_score(trues, preds, average="binary", zero_division=0)
    val_acc = accuracy_score(trues, preds)

    print(f"\nEpoch {epoch} â€” Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}\n")

    # ---------------------- EARLY STOPPING -----------------------------

    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        best_epoch = epoch
        no_improve = 0
        torch.save(model.state_dict(), "best_model.pt")
        print("Saved new best model.")
    else:
        no_improve += 1
        print(f"No improvement ({no_improve}/{patience})")
        if no_improve >= patience:
            print("Early stopping triggered.")
            break

print(f"Training complete! Best F1 = {best_val_f1:.4f} at epoch {best_epoch}")

model.eval()
bert_preds = []

with torch.no_grad():
    for batch in val_loader:
        ids, mask, _ = [b.to(device) for b in batch]
        logits = model(ids, attention_mask=mask).logits
        bert_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())

accuracy_bert  = accuracy_score(y_val, bert_preds)
precision_bert = precision_score(y_val, bert_preds)
recall_bert    = recall_score(y_val, bert_preds)
f1_bert        = f1_score(y_val, bert_preds)

print("=== DistilBERT Results ===")
print("Accuracy:", accuracy_bert)
print("Precision:", precision_bert)
print("Recall:", recall_bert)
print("F1 Score:", f1_bert)

results = pd.DataFrame({
    "Model": ["Logistic Regression", "DistilBERT"],
    "Accuracy": [accuracy_lr, accuracy_bert],
    "Precision": [precision_lr, precision_bert],
    "Recall": [recall_lr, recall_bert],
    "F1 Score": [f1_lr, f1_bert]
})

results
